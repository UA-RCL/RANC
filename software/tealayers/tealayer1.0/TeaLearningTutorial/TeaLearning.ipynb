{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TeaLearning\n",
    "==========\n",
    "\n",
    "This notebook describes the process for implementing IBM's TeaLearning training method in Keras. It assumes some basic knowledge of keras (https://keras.io) as well as the TeaLearning method itself as outlined in [\"Backpropogation for Energy-Efficient Neuromorphic Computing (Esser et al.)\"](https://papers.nips.cc/paper/5862-backpropagation-for-energy-efficient-neuromorphic-computing.pdf) and [\"Improving Classification Accuracy of Feedforward Neural Networks for Spiking Neuromorphic Chips (Yepes et al.)\"](https://www.ijcai.org/proceedings/2017/0274.pdf). The version of TeaLearning implemented in this notebook is closer to that of the latter.\n",
    "\n",
    "______________________________________________________________________________________________________________________\n",
    "\n",
    "Before we do anything else, let's import the necessary libraries. We'll be using mostly Keras, with some help from TensorFlow and Numpy. We should also import the required python library packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import operator\n",
    "import functools\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras import Model\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Activation, Input, Lambda, concatenate\n",
    "from keras.datasets import mnist\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we should look at the heart of what makes TeaLearning possible: the connections. Because of the constraints placed on weights in the TrueNorth architecture, TeaLearning focuses instead on learning connections. However, connections are binary values, and cannot be explicitly represented as floating point values. Yepes et al. circumvents this constraint by using both an *effective network* and a *shadow network*. The effective network is what actually gets deployed onto TrueNorth, with binary connections, and the the shadow network is what gets trained during backpropogation. Essentially, the shadow network represents connections as floating point values, and the effective network constrains them by setting connections to 1 if the shadow connections are greater than 0.5, and 0 otherwise. \n",
    "\n",
    "In our implementation, it may be easier to think of it this way: we have a (unconstrained) network in which we are training connections. Connections are stored and updated as floating point values. However, when we feed forward, we will constrain the connections using the constraint outlined above. \n",
    "\n",
    "As such, we will use the following TensorFlow functions to implement the constraints: `tf.round` and `tf.clip_by_value`. We will first round the connections so that they take on integer values, and then clip them between 0 and 1 so they are binary.\n",
    "\n",
    "In order to implement this during backpropogation, we need to worry about the gradients of these functions. In particular, we would like for both of these functions to assumed to have a gradient of 1 so that they don't interfere with our calculations (regardless of their actual differentiability). This behavior is default in TensorFlow for the `clip_by_value` function, but if you try to use `tf.round` during training it will throw an error (because it does not have a defined gradient).\n",
    "\n",
    "We can achieve our desired behavior using `tf.RegisterGradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.RegisterGradient(\"CustomRound\")\n",
    "def _const_round_grad(unused_op, grad):\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code tells TensorFlow how to handle the calculation for a custom gradient called \"CustomRound\". Whenever it comes across this gradient when creating its graph, it will simply return the current gradient entering the graph without altering it in any way.\n",
    "\n",
    "Now that we've gotten that out of the way, let's begin defining our class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tea(Layer):\n",
    "    def __init__(self,\n",
    "                 units,\n",
    "                 **kwargs):\n",
    "        \"\"\"Initializes a new TeaLayer.\n",
    "\n",
    "        Arguments:\n",
    "            units -- The number of neurons to use for this layer.\"\"\"\n",
    "        self.units = units\n",
    "        # Needs to be set to `True` to use the `K.in_train_phase` function.\n",
    "        self.uses_learning_phase = True\n",
    "        super(Tea, self).__init__(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Tea` class inherits from the Keras `Layer` class. For more information about Keras layers and custom layers, see https://keras.io/layers/writing-your-own-keras-layers/. \n",
    "\n",
    "The initialization of the `Tea` layer can be fairly straightforawrd. The only parameter that really needs to be defined is the number of neurons in the layer (`units`.) A full version of a `Tea` layer has many more named parameters to enable customization of the training process, but we'll keep it simple for now.\n",
    "\n",
    "The Keras Functional API allows us to abstract the fan-in and fan-out constraints of TrueNorth from the `Tea` layer. Each layer is equivalent to one `TrueNorth` core. Using lambda functions, we can handle data segmentation ourselves (we'll return to this later.)\n",
    "\n",
    "Before we move on, take note of the following line: `self.uses_learning_phase = True`. We must set this to True so that the parent class knows that we have different behavior based on whether we are training or inferring. More on that later.\n",
    "\n",
    "For now, let's implement the `build` function. This is where we initialize all of our variables. First, we need to create a helper function to initialize our weights. Because weights are not trained in TeaLearning, they are set to 1 and -1 so as to maximize information transfer. In a similar (but more simple) vein as Esser et al., we alternate between 1 and -1 for each axon. In Keras, this will look like a 2D tensor where each row represents an axon and each column represents a neuron. We will pass in a 2D shape where the first value is the number of axons and the second value is the number of neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tea_weight_initializer(shape, dtype=np.float32):\n",
    "    \"\"\"Returns a tensor of alternating 1s and -1s, which is (kind of like)\n",
    "    how IBM initializes their weight matrix in their TeaLearning\n",
    "    literature.\n",
    "\n",
    "    Arguments:\n",
    "        shape -- The shape of the weights to intialize.\n",
    "\n",
    "    Keyword Arguments:\n",
    "        dtype -- The data type to use to initialize the weights.\n",
    "                 (default: {np.float32})\"\"\"\n",
    "    num_axons = shape[0]\n",
    "    num_neurons = shape[1]\n",
    "    ret_array = np.zeros((int(num_axons), int(num_neurons)), dtype=dtype)\n",
    "    for axon_num, axon in enumerate(ret_array):\n",
    "        if axon_num % 2 == 0:\n",
    "            for i in range(len(axon)):\n",
    "                ret_array[axon_num][i] = 1\n",
    "        else:\n",
    "            for i in range(len(axon)):\n",
    "                ret_array[axon_num][i] = -1\n",
    "    return tf.convert_to_tensor(ret_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure that this funciton works. We'll run it on an example core with 10 axons and 5 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1.,  1.,  1.],\n",
       "       [-1., -1., -1., -1., -1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.],\n",
       "       [-1., -1., -1., -1., -1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.],\n",
       "       [-1., -1., -1., -1., -1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.],\n",
       "       [-1., -1., -1., -1., -1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.],\n",
       "       [-1., -1., -1., -1., -1.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.Session().run(tea_weight_initializer((10, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like everything is in being initialized correctly! On TrueNorth, each row will be an axon, and each neuron will have the same weight array: `[1, -1, 1, -1]`. The first axon (corresponding to the first row) will have an index of 0. The second axon (corresponding to the second row) will have an index of 1. The third, an index of 0, and so on.\n",
    "\n",
    "Now we can write the build function. This is pretty simple: we just need to add a variable for the weights (which are static, which is why we set `trainable=False`), the connections, and the biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(self, input_shape):\n",
    "    assert len(input_shape) >= 2\n",
    "    shape = (input_shape[-1], self.units)\n",
    "    self.static_weights = self.add_weight(\n",
    "        name='weights',\n",
    "        shape=shape,\n",
    "        initializer=tea_weight_initializer,\n",
    "        trainable=False)\n",
    "    # Intialize connections around 0.5 because they represent probabilities.\n",
    "    self.connections = self.add_weight(\n",
    "        name='connections',\n",
    "        initializer=initializers.TruncatedNormal(mean=0.5),\n",
    "        shape=shape)\n",
    "    self.biases = self.add_weight(\n",
    "        name='biases',\n",
    "        initializer='zeros',\n",
    "        shape=(self.units,))\n",
    "    super(Tea, self).build(input_shape)\n",
    "\n",
    "# Bind the method to our class\n",
    "Tea.build = build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the `call` function. This holds the feedforward code for the network.\n",
    "\n",
    "Before we do that, let's make sure we understand what the input data is going to look like. Take MNIST as an example. We have 60,000 training images, each with 784 pixels, giving us a shape of (60000, 784). That being said, each core has a fan-in of 256, so for one `Tea` layer, the input will have a shape of (60000, 256).\n",
    "\n",
    "However, we're going to add one complicaiton. In order to make up for loss of data from quantization, spiking neural networks often encode information in multiple *ticks*. If an image is encoded in 4 ticks, then there are four chances for it to spike. A simple encoding scheme would have a pixel of value zero spike no times, a pixel of value 0.5 spike twice, and a pixel of value 1 spike four times. Encoding is beyond the scope of this notebook, but we will still add the functionality to have multiple ticks to our network. We will place it in the second dimension, so that our new shape will be (60000, 1, 256). If we encoded using 4 ticks, our shape would be (60000, 4, 256). Now, all the spikes can be fed forward at once in training, and summed up at the end of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to tackle the feedforward code. So what does feeding forward look like? Let's try to visualize this. The first thing we're going to want to do is constrain our connections. Let's pretend each core only has 10 axons and 5 neurons again. This means that each partition should take 10 inputs and transform them into 5 outputs. This calls for a 10x5 matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.566906  ,  1.2955642 ,  1.29002   , -1.9821651 ,  0.89796937],\n",
       "       [ 0.16447893, -1.446152  , -0.542724  ,  0.37840036,  0.7604958 ],\n",
       "       [-0.95254433,  0.3902153 ,  1.6314734 ,  1.5750071 ,  0.7241347 ],\n",
       "       [-0.42810416,  1.3132207 ,  0.46014488, -0.361758  , -0.07966572],\n",
       "       [ 0.21166515, -0.24652898,  2.5664208 , -0.18179667, -0.00432527],\n",
       "       [ 0.92168546,  0.77903426,  0.0996834 , -0.02221549,  1.3492458 ],\n",
       "       [ 1.5381393 ,  1.4238548 , -0.5799372 , -0.4588635 , -0.4402318 ],\n",
       "       [ 1.9007677 ,  0.51779324,  0.8641211 ,  0.17820525,  0.43667778],\n",
       "       [-0.9555063 ,  1.6357096 ,  0.38619784, -0.20525771, -0.09333217],\n",
       "       [ 1.9567735 , -2.5503864 ,  0.16917345,  0.9452413 , -0.89945245]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connections = tf.keras.backend.random_normal((10, 5), mean=0.5)\n",
    "tf.Session().run(connections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we'll round and clip like we mentioned before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 1., 1.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [1., 1., 0., 0., 1.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [1., 1., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connections = tf.round(connections)\n",
    "connections = tf.clip_by_value(connections, 0, 1)\n",
    "tf.Session().run(connections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what does this matrix mean in the context of TrueNorth? Let's look at an example matrix (since we're using RNGs yours will likely differ.)\n",
    "\n",
    "```python\n",
    "array([[1., 0., 0., 0., 0.],\n",
    "       [1., 0., 1., 0., 1.],\n",
    "       [1., 1., 0., 1., 0.],\n",
    "       [1., 0., 1., 1., 1.],\n",
    "       [1., 0., 0., 0., 1.],\n",
    "       [1., 0., 0., 0., 0.],\n",
    "       [0., 0., 1., 1., 0.],\n",
    "       [0., 0., 1., 1., 0.],\n",
    "       [0., 0., 0., 0., 1.],\n",
    "       [0., 1., 0., 0., 1.]], dtype=float32)\n",
    "```\n",
    "       \n",
    "If we look at the first row, we can tell that the first axon will be connected to the 1st neuron. Similarly, the second axon will be connected to the 1st neuron, 3rd neuron, and 5th neuron. \n",
    "\n",
    "From above, we also know that the first axon will have an index of 0, the second an index of 1, the third an index of 0, and so on. This means that the first will have a weight of 1 with all neurons, and the second a weight of -1, because we are giving all neurons a weight array of `[1, -1, 1, -1]`. This means that we can just multiply our connections by our weight matrix to get the effective weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1.,  0.,  1.],\n",
       "       [-0., -0., -0., -0., -1.],\n",
       "       [ 0.,  0.,  1.,  1.,  1.],\n",
       "       [-0., -1., -0., -0., -0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.],\n",
       "       [-1., -1., -0., -0., -1.],\n",
       "       [ 1.,  1.,  0.,  0.,  0.],\n",
       "       [-1., -1., -1., -0., -0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.],\n",
       "       [-1., -0., -0., -1., -0.]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = tea_weight_initializer((10, 5))\n",
    "effective_weights = tf.multiply(connections, weights)\n",
    "tf.Session().run(effective_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets send in some input data. Because our input data will be normalized, we can take advantage of the round funciton to constrain our data to spikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.34046948, 0.74033177, 0.68914866, 0.70234466, 0.3097316 ,\n",
       "        0.44327188, 0.4642589 , 0.64185226, 0.2865076 , 0.44251502]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.keras.backend.random_uniform((1, 10), minval=0.0, maxval=1.0)\n",
    "tf.Session().run(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 1., 0., 0., 0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.round(x)\n",
    "tf.Session().run(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the axons which will receive spikes. If we had \n",
    "\n",
    "```python\n",
    "array([[1., 1., 0., 1., 0., 1., 0., 0., 0., 1.]], dtype=float32)\n",
    "```\n",
    "\n",
    "then we'd be recieving a spike on the 1st, 2nd, 4th, 6th, and 10th axons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1., -2.,  0.,  1.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tf.matmul(x, effective_weights)\n",
    "tf.Session().run(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values in this array are the current potentials for our neurons at this point. If we had \n",
    "```python\n",
    "array([[-2., -1., -2., -1., -3.]], dtype=float32)\n",
    "```\n",
    "then the 1st neuron would have a potential of -2, the second would have a potential of -1, the third a potential of -2, and so on. Now we need to apply the leak, which is equivalent to the biases in training. On TrueNorth, leak values have to be integers, so we'll round them when feeding forward, but store and update them as floating point values (like the connections.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.86214054, -0.3106843 ,  0.0547974 , -0.3456689 ,  0.00124825]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases = tf.keras.backend.random_normal((1, 5))\n",
    "tf.Session().run(biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.,  0.,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases = tf.round(biases)\n",
    "tf.Session().run(biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2., -2.,  0.,  1.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tf.add(output, biases)\n",
    "tf.Session().run(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are our final neuron potentials. Now we need to see who spikes. For TeaLearning, neurons are set with a threshold of 0, a floor of 0, and a reset potential of 0. This means that neurons reset to 0 no matter what happens, i.e., they are memoryless. Additionally, they will spike if their potential is greater than or equal to zero. Our network is already memoryless, so all we need to do is add a greater than or equal to operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cast the output because `tf.greater_equal` returns booleans\n",
    "output = tf.cast(tf.greater_equal(output, 0.0), tf.float32)\n",
    "tf.Session().run(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our final output. If we had\n",
    "\n",
    "```python\n",
    "array([[1., 1., 0., 0., 0.]], dtype=float32)\n",
    "```\n",
    "that would be equivalent to the 1st and 2nd neurons spiking. If there were four partitions, the spikes from those 5 neurons would get concatenated with the spikes from the three other sets of 5 neurons to form a 1D array of 20 outputs. This is what would get convolved over in the next layer.\n",
    "\n",
    "There's one last thing we have to worry about. During inference, it's fine to just use `tf.greater_equal`, but during training we need something that has a gradient. Following suit of Esser et al., we use the sigmoid function as this approximation. Remember in the initializer when we had the `self.uses_learning_phase = True` line? This is why we need it. During the training phase, we will feed forward using a sigmoid activaiton so that we can use backpropogation. During the test (or validaiton) phase, we will use the `tf.greater_equal` funciton be one-to-one with TrueNorth.\n",
    "\n",
    "Our `call` code should now look pretty straightforward. We replace some `tf` funcitons with `K` functions, which is the Keras backend API. Since our backend is TensorFlow, it shouldn't really make a difference, but we might as well use Keras where we can for consistency. The `K.in_train_phase` function returns the first parameter if we are in train phase, and the second parameter if we are not. This is how we can implement the sigmoid/greater_equal functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call(self, x):\n",
    "    with tf.get_default_graph().gradient_override_map(\n",
    "        {\"Round\":\"CustomRound\"}):\n",
    "        # Constrain input\n",
    "        x = tf.round(x)\n",
    "        # Constrain connections\n",
    "        connections = self.connections\n",
    "        connections = tf.round(connections)\n",
    "        connections = K.clip(connections, 0, 1)\n",
    "        # Multiply connections with weights\n",
    "        weighted_connections = connections * self.static_weights\n",
    "        # Dot input with weighted connections\n",
    "        output = K.dot(x, weighted_connections)\n",
    "        # Constrain biases\n",
    "        biases = tf.round(self.biases)\n",
    "        output = K.bias_add(\n",
    "            output,\n",
    "            biases,\n",
    "            data_format='channels_last'\n",
    "        )\n",
    "        # Apply activation / spike\n",
    "        output = K.in_train_phase(\n",
    "            K.sigmoid(output),\n",
    "            tf.cast(tf.greater_equal(output, 0.0), tf.float32)\n",
    "        )\n",
    "    return output\n",
    "    \n",
    "# Bind the method to our class\n",
    "Tea.call = call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to implement the `compute_output_shape` function. Keras uses this to help build the graph. We know that the number of neurons we have is the sum of the number of neurons in all the partitions, so we will replace the last dimesion of the input shape with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_output_shape(self, input_shape):\n",
    "    assert input_shape and len(input_shape) >= 2\n",
    "    assert input_shape[-1]\n",
    "    output_shape = list(input_shape)\n",
    "    output_shape[-1] = self.units\n",
    "    return tuple(output_shape)\n",
    "    \n",
    "# Bind the method to our class\n",
    "Tea.compute_output_shape = compute_output_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all there is to the `Tea` Layer. However, there's one complication. Because we have spiking neurons, it would be helpful to have a number of different neurons associated with each class, so that we can sum up all the guesses for each class and take the maximum. It's easy to imagine that only having one spiking neuron per class could make it very hard to make probabalistic guesses. So, we need to create a helper layer that can sum up all the neurons corresponding to a class. While we're at it, we can also up the guesses for each tick if we have multiple ticks.\n",
    "\n",
    "The code for this is pretty straight forward, and the comments should be enough to clear up anything that is unclear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditivePooling(Layer):\n",
    "    \"\"\"A helper layer designed to format data for output during TeaLearning.\n",
    "    If the data input to the layer has multiple spikes per classification, the\n",
    "    spikes for each tick are summed up. Then, all neurons that correspond to a\n",
    "    certain class are summed up so that the output is the number of spikes for\n",
    "    each class. Neurons are assumed to be arranged such that each\n",
    "    `num_classes` neurons represent a guess for each of the classes. For\n",
    "    example, if the guesses correspond to number from 0 to 9, the nuerons are\n",
    "    arranged as such:\n",
    "\n",
    "        neuron_num: 0  1  2  3  4  5  6  7  8  9  10 11 12  ...\n",
    "        guess:      0  1  2  3  4  5  6  7  8  9  0  1  2   ...\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_classes,\n",
    "                 **kwargs):\n",
    "        \"\"\"Initializes a new `AdditivePooling` layer.\n",
    "\n",
    "        Arguments:\n",
    "            num_classes -- The number of classes to output.\n",
    "        \"\"\"\n",
    "        self.num_classes = num_classes\n",
    "        self.num_inputs = None\n",
    "        super(AdditivePooling, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 2\n",
    "        # The number of neurons must be collapsable into the number of classes\n",
    "        assert input_shape[-1] % self.num_classes == 0\n",
    "        self.num_inputs = input_shape[-1]\n",
    "\n",
    "    def call(self, x):\n",
    "        # Sum up ticks if there are ticks\n",
    "        if len(x.shape) >= 3:\n",
    "            output = K.sum(x, axis=1)\n",
    "        else:\n",
    "            output = x\n",
    "        # Reshape output\n",
    "        output = tf.reshape(\n",
    "            output,\n",
    "            [-1, int(self.num_inputs / self.num_classes), self.num_classes]\n",
    "        )\n",
    "        # Sum up neurons\n",
    "        output = tf.reduce_sum(output, 1)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_shape = list(input_shape)\n",
    "        # Last dimension will be number of classes\n",
    "        output_shape[-1] = self.num_classes\n",
    "        # Ticks were summed, so delete tick dimension if exists\n",
    "        if len(output_shape) >= 3:\n",
    "            del output_shape[1]\n",
    "        return tuple(output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're finally ready to do some training! Let's start by loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# save old labels for later\n",
    "y_test_not = y_test\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now we'll define the model using the Keras Functional API. We'll use a Lambda layer to seperate our input into 4 different partitions of 256 pixels each so that each partition can be fed into one core. Then, we'll send them into Tea layers with 64 neurons each. Those 4\\*64 outputs will be concatenated into one array of 256 values, which we'll send into an output core. The output core will have 250 neurons, which works out to be 25 neurons guessing per class (since we have 10 classes.) We'll send all of this into the AdditivePooling layer, which will output 10 values (one for each class) which we'll finally feed into a softmax function to aide training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define model (use functional API to follow fan-in and \n",
    "# fan-out constraints)\n",
    "inputs = Input(shape=(28, 28,))\n",
    "flattened_inputs = Flatten()(inputs)\n",
    "# Send input into 4 different cores (256 axons each)\n",
    "x0 = Lambda(lambda x : x[:,:256])(flattened_inputs)\n",
    "x1 = Lambda(lambda x : x[:,176:432])(flattened_inputs)\n",
    "x2 = Lambda(lambda x : x[:,352:608])(flattened_inputs)\n",
    "x3 = Lambda(lambda x : x[:,528:])(flattened_inputs)\n",
    "x0 = Tea(64)(x0)\n",
    "x1 = Tea(64)(x1)\n",
    "x2 = Tea(64)(x2)\n",
    "x3 = Tea(64)(x3)\n",
    "# Concatenate output of first layer to send into next\n",
    "x = concatenate([x0, x1, x2, x3])\n",
    "x = Tea(250)(x)\n",
    "# Pool spikes and output neurons into 10 classes.\n",
    "x = AdditivePooling(10)(x)\n",
    "predictions = Activation('softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/5\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.5806 - acc: 0.8153 - val_loss: 0.4347 - val_acc: 0.8610\n",
      "Epoch 2/5\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.2902 - acc: 0.9107 - val_loss: 0.3639 - val_acc: 0.8840\n",
      "Epoch 3/5\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.2299 - acc: 0.9286 - val_loss: 0.2967 - val_acc: 0.9053\n",
      "Epoch 4/5\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.2026 - acc: 0.9373 - val_loss: 0.2807 - val_acc: 0.9116\n",
      "Epoch 5/5\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1820 - acc: 0.9426 - val_loss: 0.2706 - val_acc: 0.9133\n",
      "Test loss: 0.2618567518532276\n",
      "Test accuracy: 0.9143\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=5,\n",
    "          verbose=1,\n",
    "          validation_split=0.2)\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a working network, we can use the `truenorthutils` package to output this network for the TrueNorth simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from truenorthutils.simulator import simulator_conversion\n",
    "from truenorthutils.tealearning import create_cores, create_packets, Packet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To output for the simulator, we need to extract the connections, and biases from the network. We can use the `model.get_weights()` function to do this. This function returns a list of all the variables of the model. For each layer, the first variable is the connections, the second is the biases, and the third is the weights. `create_cores` expects a 2D list, where each inner list is a conceptual layer, and each item in the list is a core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_weights = model.get_weights()\n",
    "connections = [\n",
    "    [np.clip(np.round(all_weights[0]), 0, 1).astype(int),\n",
    "     np.clip(np.round(all_weights[3]), 0, 1).astype(int),\n",
    "     np.clip(np.round(all_weights[6]), 0, 1).astype(int),\n",
    "     np.clip(np.round(all_weights[9]), 0, 1).astype(int)],\n",
    "    [np.clip(np.round(all_weights[12]), 0, 1).astype(int)]\n",
    "]\n",
    "biases = [\n",
    "    [np.round(all_weights[1]).astype(int), \n",
    "     np.round(all_weights[4]).astype(int),\n",
    "     np.round(all_weights[7]).astype(int),\n",
    "     np.round(all_weights[10]).astype(int)],\n",
    "    [np.round(all_weights[13]).astype(int)]\n",
    "]\n",
    "weights = [[all_weights[2], all_weights[5], all_weights[8], all_weights[11]], [all_weights[14]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = create_cores(connections, weights, biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll iterate through the first 10 images and create a 2D list of packet objects to be sent in to the simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coordinates of input core\n",
    "core_coordinates = [(0, 0), (1, 0), (2, 0), (3, 0)]\n",
    "# Convert the first 10 images to packets \n",
    "packets = []\n",
    "for image_num, image in enumerate(x_test[:10]):\n",
    "    temp_packets = []\n",
    "    # Flatten image\n",
    "    image = image.reshape(784)\n",
    "    for i in range(0, 4):\n",
    "        for pixel_num, pixel in enumerate(image[176*i:176*i+256]):\n",
    "            if pixel > 0.5:\n",
    "                temp_packets.append(\n",
    "                    Packet(\n",
    "                        core_coordinates[i],\n",
    "                        pixel_num,\n",
    "                        0\n",
    "                    )\n",
    "                )\n",
    "    packets.append(temp_packets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, this function creats the `input.txt` and `labels.txt` functions for the simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator_conversion(cores, packets=packets, labels=y_test_not)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the simulator with these files using the following command (within the `SoftwareSimulator/python_wrapper` directory\n",
    "\n",
    "`python driver.py PATH_TO_INPUT/input.txt PATH_TO_LABELS/labels.txt PATH_FOR_OUTPUT/output.txt -t 11 -r 1`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
